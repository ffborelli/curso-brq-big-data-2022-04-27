{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c384c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura os nós de um cluster (nesse caso estamos rodando standalone)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3540c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura os nós de um cluster (nesse caso estamos rodando standalone)\n",
    "spark = SparkSession.builder.appName(\"aula-pyspark\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ed9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de um CSV e permitindo que o Spark infira o tipo de dados. Também informamos que o arquivo contém um header\n",
    "df = spark.read.csv('ABT.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1e9dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificando o tipo de dados da variável *df*\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d99fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- adjclose: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# monstrando o schema do arquivo que importamos \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83607ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'volume', 'open', 'high', 'low', 'close', 'adjclose']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostrar colunas do Data Frame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c31ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|      date| volume|            open|             high|              low|            close|         adjclose|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2020-07-02|3845300|            92.5|93.05999755859375|91.93000030517578| 92.2300033569336| 92.2300033569336|\n",
      "|2020-07-01|3389600|91.9800033569336| 91.9800033569336|90.43000030517578|91.63999938964844|91.63999938964844|\n",
      "|2020-06-30|5220900|           88.75| 91.9000015258789|88.44000244140625|91.43000030517578|91.43000030517578|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostrar os 3 primeiros dados do data frame\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ece6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|      date| volume|             open|             high|              low|            close|         adjclose|\n",
      "+----------+-------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2020-07-01|3389600| 91.9800033569336| 91.9800033569336|90.43000030517578|91.63999938964844|91.63999938964844|\n",
      "|2020-06-30|5220900|            88.75| 91.9000015258789|88.44000244140625|91.43000030517578|91.43000030517578|\n",
      "|2020-06-29|4669300|89.58000183105469|89.69999694824219|88.08999633789062|89.01000213623047|89.01000213623047|\n",
      "+----------+-------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter -> permite filtrar dados do data frame e mostrar apenas os 3 primeiros registros\n",
    "df.filter(\"open<92\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba4b3dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|      date| volume|            open|             high|              low|            close|         adjclose|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|2020-07-02|3845300|            92.5|93.05999755859375|91.93000030517578| 92.2300033569336| 92.2300033569336|\n",
      "|2020-07-01|3389600|91.9800033569336| 91.9800033569336|90.43000030517578|91.63999938964844|91.63999938964844|\n",
      "|2020-06-30|5220900|           88.75| 91.9000015258789|88.44000244140625|91.43000030517578|91.43000030517578|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter -> permite filtrar dados do data frame e mostrar apenas os 3 primeiros registros\n",
    "df.filter( (df[\"open\"] < 93)  ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9884da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|            open|            close|\n",
      "+----------------+-----------------+\n",
      "|            92.5| 92.2300033569336|\n",
      "|91.9800033569336|91.63999938964844|\n",
      "|           88.75|91.43000030517578|\n",
      "+----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter -> permite filtrar dados do data frame -> mostrar apenas duas colunas\n",
    "df.filter( (df[\"open\"] < 93)  ).select([\"open\",\"close\"]).show(3)\n",
    "df.select([\"open\",\"close\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c8ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma nova coluna -> df é sempre imutável -> então devemos criar outra coluna em um outro dataframe\n",
    "dfCloseOpen = df.withColumn(\"close/open\" , df[\"close\"]/df[\"open\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc8a8c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificando o tipo de dados da variável *dfCloseOpen*\n",
    "type(dfCloseOpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e912fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+------------------+\n",
      "|            close|            open|        close/open|\n",
      "+-----------------+----------------+------------------+\n",
      "| 92.2300033569336|            92.5| 0.997081117372255|\n",
      "|91.63999938964844|91.9800033569336|0.9963035012516172|\n",
      "|91.43000030517578|           88.75| 1.030197186537192|\n",
      "+-----------------+----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostrar apenas as colunas \"close\", \"open\", \"close/open\" -> e os 3 primeiros registros\n",
    "dfCloseOpen.select([\"close\", \"open\", \"close/open\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as funções max e min \n",
    "from pyspark.sql.functions import max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "652c8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   max(close/open)|\n",
      "+------------------+\n",
      "|1.1166077691829603|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrar o processamento da função max, dividindo as colunas close/open -> apenas 3 primeiros resultados\n",
    "dfCloseOpen.select(max(\"close/open\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a72eb1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   min(close/open)|\n",
      "+------------------+\n",
      "|0.9078590266226547|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrar o processamento da função min, dividindo as colunas close/open -> apenas 3 primeiros resultados\n",
    "dfCloseOpen.select(min(\"close/open\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79b7178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando when e col para processar os dados da coluna flag\n",
    "from pyspark.sql.functions import when,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7265aead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#novo dataframe -> com a coluna flag, com um IF-ELSE -> IF \"close/open\" > 1: 1 ---ELSE 0\n",
    "dfCloseOpenCau = dfCloseOpen.withColumn ('flag', when( col(\"close/open\") > 1, 1 ).otherwise(0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83bf2309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|        close/open|flag|\n",
      "+------------------+----+\n",
      "| 0.997081117372255|   0|\n",
      "|0.9963035012516172|   0|\n",
      "| 1.030197186537192|   1|\n",
      "+------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando apenas 3 dados das colunas \"close/open\",\"flag\"\n",
    "dfCloseOpenCau.select([\"close/open\",\"flag\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cda460e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando a função Year\n",
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a73c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(date)|\n",
      "+----------+\n",
      "|      2020|\n",
      "|      2020|\n",
      "|      2020|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando apenas o ano da função YEAR\n",
    "dfCloseOpen.select ( year( dfCloseOpen[\"date\"] ) ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd091e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#atualizando dataframe para armazenar a coluna YEAR\n",
    "dfCloseOpen = dfCloseOpen.withColumn (\"Year\", year( dfCloseOpen[\"date\"] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ad163f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      date|Year|\n",
      "+----------+----+\n",
      "|2020-07-02|2020|\n",
      "|2020-07-01|2020|\n",
      "|2020-06-30|2020|\n",
      "+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando as colunas date e Year, 3 primeiros registros\n",
    "dfCloseOpen.select(\"date\",\"Year\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "135686a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando a função Month\n",
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72dc9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostrando apenas o ano da função MONTH\n",
    "dfCloseOpen = dfCloseOpen.withColumn (\"Month\", month( dfCloseOpen[\"date\"] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca7d0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|      date|Month|Year|\n",
      "+----------+-----+----+\n",
      "|2020-07-02|    7|2020|\n",
      "|2020-07-01|    7|2020|\n",
      "|2020-06-30|    6|2020|\n",
      "+----------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando as colunas date, Month e Year, 3 primeiros registros\n",
    "dfCloseOpen.select(\"date\",\"Month\",\"Year\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ad96b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando função dia do mês\n",
    "from pyspark.sql.functions import dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37be6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#atualizando dataframe para armazenar a coluna Day\n",
    "dfCloseOpen = dfCloseOpen.withColumn (\"Day\", dayofmonth( dfCloseOpen[\"date\"] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e9ff5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+----+\n",
      "|      date|Day|Month|Year|\n",
      "+----------+---+-----+----+\n",
      "|2020-07-02|  2|    7|2020|\n",
      "|2020-07-01|  1|    7|2020|\n",
      "|2020-06-30| 30|    6|2020|\n",
      "+----------+---+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando as colunas date, Month, Year e Day, 3 primeiros registros\n",
    "dfCloseOpen.select(\"date\",\"Day\",\"Month\",\"Year\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd22591b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`Year`' given input columns: [Month, adjclose, close, close/open, date, high, low, open, volume];\n'Aggregate ['Year], ['Year, max(volume#17) AS max(volume)#375, max(open#18) AS max(open)#376, max(high#19) AS max(high)#377, max(low#20) AS max(low)#378, max(close#21) AS max(close)#379, max(adjclose#22) AS max(adjclose)#380, max(close/open#212) AS max(close/open)#381, max(Month#348) AS max(Month)#382]\n+- Project [date#16, volume#17, open#18, high#19, low#20, close#21, adjclose#22, close/open#212, month(cast(date#16 as date)) AS Month#348]\n   +- Project [date#16, volume#17, open#18, high#19, low#20, close#21, adjclose#22, (close#21 / open#18) AS close/open#212]\n      +- Relation[date#16,volume#17,open#18,high#19,low#20,close#21,adjclose#22] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-30ad68edf446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#agrupando dados por ano e o resultado deste agrupamento deve ser o valor máximo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfGroupByMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfCloseOpen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`Year`' given input columns: [Month, adjclose, close, close/open, date, high, low, open, volume];\n'Aggregate ['Year], ['Year, max(volume#17) AS max(volume)#375, max(open#18) AS max(open)#376, max(high#19) AS max(high)#377, max(low#20) AS max(low)#378, max(close#21) AS max(close)#379, max(adjclose#22) AS max(adjclose)#380, max(close/open#212) AS max(close/open)#381, max(Month#348) AS max(Month)#382]\n+- Project [date#16, volume#17, open#18, high#19, low#20, close#21, adjclose#22, close/open#212, month(cast(date#16 as date)) AS Month#348]\n   +- Project [date#16, volume#17, open#18, high#19, low#20, close#21, adjclose#22, (close#21 / open#18) AS close/open#212]\n      +- Relation[date#16,volume#17,open#18,high#19,low#20,close#21,adjclose#22] csv\n"
     ]
    }
   ],
   "source": [
    "#agrupando dados por ano e o resultado deste agrupamento deve ser o valor máximo\n",
    "dfGroupByMax = dfCloseOpen.groupBy(\"Year\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40c2db49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfGroupByMax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8815f5ef090a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ordenar os dados por ano, mostrar apenas colunas \"Year\", \"max(close)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfGroupByMax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max(close)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfGroupByMax' is not defined"
     ]
    }
   ],
   "source": [
    "# ordenar os dados por ano, mostrar apenas colunas \"Year\", \"max(close)\"\n",
    "from pyspark.sql.functions import col, desc\n",
    "dfGroupByMax.sort( col(\"Year\").desc() ).select([\"Year\", \"max(close)\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e088c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+-----------------+----+-----+---+\n",
      "|      date| volume|open|             high|              low|           close|        adjclose|       close/open|Year|Month|Day|\n",
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+-----------------+----+-----+---+\n",
      "|2020-07-02|3845300|92.5|93.05999755859375|91.93000030517578|92.2300033569336|92.2300033569336|0.997081117372255|2020|    7|  2|\n",
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+-----------------+----+-----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrar apenas uma linha com todas as colunas\n",
    "dfCloseOpen.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb22f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#escrevendo em Parquet na pasta /home/jovyan/parquet/\n",
    "dfCloseOpen.write.mode(\"overwrite\").parquet(\"/home/jovyan/parquet/ABT.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77a1ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo o arquivo Parquet e colocando no dataframe *dfParquet*\n",
    "dfParquet = spark.read.parquet(\"/home/jovyan/parquet/ABT.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b9c50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+------------------+----+-----+---+\n",
      "|      date| volume|            open|             high|              low|            close|         adjclose|        close/open|Year|Month|Day|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+------------------+----+-----+---+\n",
      "|2020-07-02|3845300|            92.5|93.05999755859375|91.93000030517578| 92.2300033569336| 92.2300033569336| 0.997081117372255|2020|    7|  2|\n",
      "|2020-07-01|3389600|91.9800033569336| 91.9800033569336|90.43000030517578|91.63999938964844|91.63999938964844|0.9963035012516172|2020|    7|  1|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+------------------+----+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrar apenas duas linhas com todas as colunas (dataFrame Parquet)\n",
    "dfParquet.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "929ad8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando SQLContect para poder manipular o Dataframe como se fosse uma Tabela SQL\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5bf5e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# createOrReplaceTempView -> criar uma visão temporária com o nome myTable (visão permite acessar os dados como uma tabela SQL)\n",
    "dfParquet.createOrReplaceTempView(\"myTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5edc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from myTable where close > 92\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef22ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+-----------------+----+-----+---+\n",
      "|      date| volume|            open|             high|              low|            close|         adjclose|       close/open|Year|Month|Day|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+-----------------+----+-----+---+\n",
      "|2020-07-02|3845300|            92.5|93.05999755859375|91.93000030517578| 92.2300033569336| 92.2300033569336|0.997081117372255|2020|    7|  2|\n",
      "|2020-06-10|5829100|90.8499984741211|            92.75|90.63999938964844|92.16000366210938|92.16000366210938|1.014419429939357|2020|    6| 10|\n",
      "|2020-06-08|5426800|88.9800033569336|92.61000061035156|88.91000366210938|92.55999755859375|92.55999755859375|1.040233693713175|2020|    6|  8|\n",
      "+----------+-------+----------------+-----------------+-----------------+-----------------+-----------------+-----------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostra os 3 primeiros dados do resultado SQL acima\n",
    "sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a68d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, volume: int, open: double, high: double, low: double, close: double, adjclose: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXTRA\n",
    "#remover valores nulos de todas as colunas\n",
    "df.na.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4633eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, volume: int, open: double, high: double, low: double, close: double, adjclose: double]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXTRA\n",
    "#remover valores nulos apenas das colunas 'open', 'close'\n",
    "df.dropna(subset = [\"open\", \"close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Deaths` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829a56e",
   "metadata": {},
   "source": [
    "# Aula dia 1 de junho de 2021\n",
    "## Tópicos\n",
    "1. UnionAll (Dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7d9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b106fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"unionall-pyspark\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8e2acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAAN = spark.read.csv('AAN.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050378c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|      date|volume|             open|             high|               low|            close|         adjclose|\n",
      "+----------+------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|2020-07-02|413000|46.79999923706055|47.41999816894531|44.540000915527344|44.72999954223633|44.72999954223633|\n",
      "+----------+------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAAN.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e940075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfABT = spark.read.csv('ABT.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c5557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+\n",
      "|      date| volume|open|             high|              low|           close|        adjclose|\n",
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+\n",
      "|2020-07-02|3845300|92.5|93.05999755859375|91.93000030517578|92.2300033569336|92.2300033569336|\n",
      "+----------+-------+----+-----------------+-----------------+----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfABT.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1c53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando DataFrame para poder fazer a união dos DataFrames\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ad683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUnionAll = DataFrame.unionAll(dfABT, dfAAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4333598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'volume', 'open', 'high', 'low', 'close', 'adjclose']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUnionAll.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479c988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para colocar valores fixos dentro de uma coluna\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "543884f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adicionando uma nova coluna dentro da DataFrame\n",
    "dfABTSymbol = dfABT.withColumn('symbol', lit('ABT') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "972f897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adicionando uma nova coluna dentro da DataFrame\n",
    "dfAANSymbol = dfAAN.withColumn('symbol', lit('AAN') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d532aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junção dos DataFrames com unionAll\n",
    "dfUnionAll = DataFrame.unionAll(dfABTSymbol, dfAANSymbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9a12ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'volume', 'open', 'high', 'low', 'close', 'adjclose', 'symbol']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUnionAll.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "204c80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|symbol|\n",
      "+------+\n",
      "|   AAN|\n",
      "|   ABT|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrando os valores distintos da coluna 'symbol'\n",
    "dfUnionAll.select('symbol').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c7c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
