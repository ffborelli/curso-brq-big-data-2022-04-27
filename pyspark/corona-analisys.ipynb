{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cee00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura os nós de um cluster (nesse caso estamos rodando standalone)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "535ecb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura os nós de um cluster (nesse caso estamos rodando standalone)\n",
    "spark = SparkSession.builder.appName(\"corona-analisys\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d11dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de um CSV e permitindo que o Spark infira o tipo de dados. Também informamos que o arquivo contém um header\n",
    "df = spark.read.csv('covid_19_data.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29b4099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|    Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "|  1|     01/22/2020|         Anhui|Mainland China|1/22/2020 17:00|      1.0|   0.0|      0.0|\n",
      "|  2|     01/22/2020|       Beijing|Mainland China|1/22/2020 17:00|     14.0|   0.0|      0.0|\n",
      "|  3|     01/22/2020|     Chongqing|Mainland China|1/22/2020 17:00|      6.0|   0.0|      0.0|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# listando 3 linhas para ver se deu certo a importação do CSV\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdae0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando SQLContect para poder manipular o Dataframe como se fosse uma Tabela SQL\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e9afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# createOrReplaceTempView -> criar uma visão temporária com o nome coronaTable (visão permite acessar os dados como uma tabela SQL)\n",
    "df.createOrReplaceTempView(\"coronaTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "542ce5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Province/State` is null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2f13fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando when e col para processar os dados da coluna flag\n",
    "from pyspark.sql.functions import when,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ca2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando outro DataFrame (sobrescrevendo ) o DataFrame anterior, alteramos a coluna 'Province/State' quando ela possuísse valor null para 'N/A', caso contrário, retornamos o valor já existente\n",
    "df = df.withColumn( 'Province/State', when ( col ('Province/State').isNull() , 'N/A' ).otherwise(df['Province/State'])   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d6f122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtrando a coluna 'Province/State' para verificar se ainda possui valores nulos\n",
    "df.filter ( df['Province/State'].isNull() ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2407d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `ObservationDate` is null\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a873a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Country/Region` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11be43fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Last Update` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "878c8333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Confirmed` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced06fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Deaths` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2374ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Recovered` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "759ad999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "+---+---------------+--------------+--------------+-----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executa uma consulta SQL no dataframe (na verdade visão criada na linha acima)\n",
    "sql = spark.sql(\"select * from coronaTable where `Deaths` is null\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6049e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preencher com valores 0 a coluna 'Deaths' quando houver valor null\n",
    "df.na.fill(0,subset=['Deaths']).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a207919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aleph - dúvida de agregação do novo campo\n",
    "dfGroupByMax = df.groupBy(\"Province/State\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44a6f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenar os dados \n",
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdce7b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------------+-----------+--------------+\n",
      "|Province/State|max(SNo)|max(Confirmed)|max(Deaths)|max(Recovered)|\n",
      "+--------------+--------+--------------+-----------+--------------+\n",
      "|       England|  284851|     3861901.0|   112182.0|           0.0|\n",
      "|           N/A|  284714|     5605532.0|   104093.0|     4480381.0|\n",
      "|     Sao Paulo|  285171|     2923367.0|    97058.0|     2588973.0|\n",
      "|   Maharashtra|  285006|     4665754.0|    69615.0|     3930302.0|\n",
      "|    California|  284792|     3744830.0|    62078.0|           6.0|\n",
      "|      New York|  285064|     2054848.0|    52358.0|           0.0|\n",
      "|         Texas|  285216|     2897110.0|    50290.0|           0.0|\n",
      "|Rio de Janeiro|  285147|      747449.0|    44835.0|      694160.0|\n",
      "|       Florida|  284858|     2242778.0|    35268.0|           0.0|\n",
      "|  Minas Gerais|  285029|     1370202.0|    34289.0|     1256330.0|\n",
      "+--------------+--------+--------------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dúvida Gabriel\n",
    "dfGroupByMax.sort( col(\"max(Deaths)\").desc() ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03495bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import format number para formatar a quantidade de casas decimais uma coluna\n",
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bf4ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|  gabriel|Province/State|\n",
      "+---------+--------------+\n",
      "|        0|       England|\n",
      "|4,480,381|           N/A|\n",
      "|2,588,973|     Sao Paulo|\n",
      "|3,930,302|   Maharashtra|\n",
      "|        6|    California|\n",
      "|        0|      New York|\n",
      "|        0|         Texas|\n",
      "|  694,160|Rio de Janeiro|\n",
      "|        0|       Florida|\n",
      "|1,256,330|  Minas Gerais|\n",
      "+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dúvida Gabriel\n",
    "dfGroupByMax.sort( col(\"max(Deaths)\").desc() ).select(format_number('max(Recovered)', 0).alias('gabriel') , 'Province/State').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2196d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vitor - dúvida de agregação do novo campo\n",
    "dfGroupByCount = df.groupBy(\"Country/Region\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c6e5689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Country/Region|count|\n",
      "+--------------+-----+\n",
      "|          Chad|  410|\n",
      "|        Russia|28010|\n",
      "|      Paraguay|  421|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfGroupByCount.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dd09803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando a função Year\n",
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb7b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------------+-----------+--------------+\n",
      "|Province/State|max(SNo)|max(Confirmed)|max(Deaths)|max(Recovered)|\n",
      "+--------------+--------+--------------+-----------+--------------+\n",
      "|          Utah|  285254|      398012.0|     2204.0|           0.0|\n",
      "|     Cajamarca|  284789|       52187.0|     1255.0|           0.0|\n",
      "|       Antwerp|  284740|      127330.0|        0.0|           0.0|\n",
      "+--------------+--------+--------------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfGroupByMax.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "985c2563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|    Last Update|Confirmed|Deaths|Recovered|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "|  1|     01/22/2020|         Anhui|Mainland China|1/22/2020 17:00|      1.0|   0.0|      0.0|\n",
      "|  2|     01/22/2020|       Beijing|Mainland China|1/22/2020 17:00|     14.0|   0.0|      0.0|\n",
      "|  3|     01/22/2020|     Chongqing|Mainland China|1/22/2020 17:00|      6.0|   0.0|      0.0|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5673397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|year(ObservationDate)|\n",
      "+---------------------+\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select ( year (\"ObservationDate\" ) ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8808876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solução da Nilane\n",
    "from pyspark.sql.functions import unix_timestamp, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0085d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funçao que converte a data do formato 'MM/dd/yyyy' para timestamp\n",
    "#strDate é um parâmetro com a data no formato 'MM/dd/yyyy' -> Ex: '01/22/2020'\n",
    "# convertendo a data no formato 'MM/dd/yyyy' para unix timestamp (quantos segundos passaram do dia 1 de janeiro de 1970 até a data alvo)\n",
    "# unix_timestamp( col(\"ObservationDate\"), 'MM/dd/yyyy' ) -> retorna um objeto do tipo unix_timestamp\n",
    "# to_date() espera um objeto do tipo timestamp, então devemos 'converter' de unix_timestamp para timestamp -> cast('timestamp')\n",
    "#retornat a data no formato Timestamp - Ex : 2020-01-22\n",
    "def convertStrToTimestamp (strDate, formatDate = 'MM/dd/yyyy' ):\n",
    "    ut = unix_timestamp( strDate, formatDate )\n",
    "    ts = ut.cast('timestamp')\n",
    "    return to_date ( ts )\n",
    "# convertStrToTimestamp( '01/01/2020', 'yyyy-dd-MM')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2780d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|year(to_date(CAST(unix_timestamp(ObservationDate, MM/dd/yyyy) AS TIMESTAMP)))|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|                                                                         2020|\n",
      "|                                                                         2020|\n",
      "|                                                                         2020|\n",
      "+-----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostrando os dados da coluna 'ObservationDate', já convertidos em ano\n",
    "df.select ( year( convertStrToTimestamp( col('ObservationDate')  ) ) ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f803df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando o dataFrame *df*, iremos adicionar uma nova coluna apenas com o valor de Ano(Year)\n",
    "df = df.withColumn ( 'Year', year( convertStrToTimestamp( col('ObservationDate')  ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f96e33bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+----+\n",
      "|SNo|ObservationDate|Province/State|Country/Region|    Last Update|Confirmed|Deaths|Recovered|Year|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+----+\n",
      "|  1|     01/22/2020|         Anhui|Mainland China|1/22/2020 17:00|      1.0|   0.0|      0.0|2020|\n",
      "|  2|     01/22/2020|       Beijing|Mainland China|1/22/2020 17:00|     14.0|   0.0|      0.0|2020|\n",
      "|  3|     01/22/2020|     Chongqing|Mainland China|1/22/2020 17:00|      6.0|   0.0|      0.0|2020|\n",
      "+---+---------------+--------------+--------------+---------------+---------+------+---------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c18cf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando a função Month\n",
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a50d59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando o dataFrame *df*, iremos adicionar uma nova coluna apenas com o valor de Mês(Month)\n",
    "df = df.withColumn ( 'Month', month( convertStrToTimestamp( col('ObservationDate')  ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51c72a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrupando por ano, mês -> retorando os valores máximos das outras colunas\n",
    "dfGroupBy = df.groupBy('Year', 'Month', 'Country/Region', 'Province/State').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f5b2532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------+----------------+--------+--------------+-----------+--------------+---------+----------+\n",
      "|Year|Month|Country/Region|  Province/State|sum(SNo)|sum(Confirmed)|sum(Deaths)|sum(Recovered)|sum(Year)|sum(Month)|\n",
      "+----+-----+--------------+----------------+--------+--------------+-----------+--------------+---------+----------+\n",
      "|2021|    5|            UK|         England|  568938|     7722425.0|   224351.0|           0.0|     4042|        10|\n",
      "|2021|    5|        France|             N/A|  568432|   1.1202827E7|   208100.0|      614539.0|     4042|        10|\n",
      "|2021|    5|        Brazil|       Sao Paulo|  569578|     5841411.0|   193999.0|     5141626.0|     4042|        10|\n",
      "|2021|    5|          Iran|             N/A|  568466|     5051012.0|   144574.0|     3959229.0|     4042|        10|\n",
      "|2021|    5|         India|     Maharashtra|  569248|     9331508.0|   139230.0|     7860604.0|     4042|        10|\n",
      "|2021|    5|        Poland|             N/A|  568568|     5601850.0|   135992.0|     5031826.0|     4042|        10|\n",
      "|2021|    5|     Argentina|             N/A|  568336|     5999124.0|   128348.0|     5331556.0|     4042|        10|\n",
      "|2021|    5|            US|      California|  568820|     7488511.0|   123805.0|           0.0|     4042|        10|\n",
      "|2021|    5|  South Africa|             N/A|  568610|     3166906.0|   108823.0|     3014510.0|     4042|        10|\n",
      "|2021|    5|            US|        New York|  569364|     4106870.0|   104667.0|           0.0|     4042|        10|\n",
      "|2021|    5|            US|           Texas|  569668|     5792920.0|   100564.0|           0.0|     4042|        10|\n",
      "|2021|    5|     Indonesia|             N/A|  568464|     3350154.0|    91448.0|     3057696.0|     4042|        10|\n",
      "|2021|    5|        Brazil|  Rio de Janeiro|  569530|     1492905.0|    89454.0|     1383315.0|     4042|        10|\n",
      "|2021|    5|        Turkey|             N/A|  568642|     9724796.0|    81348.0|     8885580.0|     4042|        10|\n",
      "|2021|    5|            US|         Florida|  568952|     4481715.0|    70507.0|           0.0|     4042|        10|\n",
      "|2021|    5|        Brazil|    Minas Gerais|  569294|     2736804.0|    68325.0|     2512660.0|     4042|        10|\n",
      "|2021|    5|        Mexico|          Mexico|  569286|      489167.0|    68006.0|           0.0|     4042|        10|\n",
      "|2021|    5|         Italy|       Lombardia|  569218|     1614805.0|    65867.0|     1445029.0|     4042|        10|\n",
      "|2021|    5|        Mexico|Ciudad de Mexico|  568884|     1283556.0|    65199.0|           0.0|     4042|        10|\n",
      "|2021|    5|Czech Republic|             N/A|  568400|     3267046.0|    58659.0|     3110367.0|     4042|        10|\n",
      "+----+-----+--------------+----------------+--------+--------------+-----------+--------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ordenando dataframe pela coluna 'max(Deaths)' de forma descendente\n",
    "dfGroupBy.sort(col('Year').desc(),col('Month').desc(),  col('sum(Deaths)').desc(), col('Province/State') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607d7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
